# Topic: Programming Languages and Calculi
## Speaker: Matthias Felleisen

(Mitch had to cancel, Matthias is filling in. Unclear if Mitch will lecture later.)

### 1984

Boom in functional programming.
Of course the in-fighting started the year before.

Group A: FP is good because it has:
- the λ calculus
- Church-Rosser
- referential transparency

Group B: polluted that wonderful picture
- added ref cells, `set!`
- exceptions, continuations

> O: these people are still around! Same two groups.
>    Group B just makes stuff up, Group A tends to lag behind, and that
>    makes it hard to synthesize.

> J: why do you put continuations in the "impure" set?

> M: because it is impure, we'll get to that

State was a problem for group A. The group A view of the world was the
Milne/Strachey extension of the Scott/Strachey denotational semantics.
Couldn't handle state, no `Σ`. They tried to provde the store away, but couldn't.

I arrived in Jan 1984, fell in love with `call/cc`.
That's an operation that grabs the current continutation. Very unusual.
Exists in Smalltalk, FP, (has different names in different places).
C eventually got `setjmp` and `setlongjmp`

???

Called it `prompt` and got teased.
See it reminded me of the REPL prompt, and I didn't want control to escape the REPL toplevel,
but that's falling for the syntax, and not for the sematic meaning of the thing.
So I waited 4 years to publish the paper on "prompt"

???

advisor says, i have an idea. he wrote:
```
  (f (call/cc h)) ~= h(f)
```
isn't that cool! That must be like `β`. Go implement it!
Typical Dan style.

I thought about it.
Sure, if I'm asking for the rest of the computation after `h`, then `f`
is exactly that.

If you have 2 `f`s, like `(g (f (call/cc h)))` it works too, that's easy `h(f ∘ g)`

what about `(call/cc h) e`? well that's `h(λx.x e)`

what about `add1(call/cc (λcont . add1(cont 0)))`?
should be
```
(λcont . add1 (cont 0)) add1
=
(add1 (add1 0))
=
2
```

but if you run the program you get 2.
The problem is, the computation ends at the end of the continuation.
Need to add `stop` and `abort` delimiters.


I started playing with it in Austin Texas. Had a child, didn't want a Hoosier,
so we moved to Texas.

Dan goes back, talks to Bruce Duba & Eugene Kohlbecker. They figure out the
same things I did in Texas, we realize the original equation was wrong.
It only works if you have the WHOLE program sitting there! That's not an
equation. Equations can be applied anywhere!

Didn't know what to do.

Luckily I remembered reading a paper. Totally impenetrable paper.
But first I need to give some history, and this history will reveal something
about group A.

> O: what I'm getting from this story is a guy that started in group B,
>    moved over to group A,
>    but refused to give up his favorite toys

> M: yep

#### 1928

Goedel visited Church, intented the λ calculus.
They were studying functions. Pure functions. How far can we go modeling
computation with just functions? (spoiler: pretty far)

1950-ish,
- Boehm, CUCH
- Landin SECD

Algol '60. Question: what does ALGOL '60 mean? Lots of people tried answering
- McCarthy
- Landin
- Reynolds

Idea: this λ calculus thing could be a programming language!
So McCarthy has papers, disjoint from his LISP thing, where he says we should
program with recursive functions

> O: but he's not really higher order

> M: ehhh

Another thing, the denotational semantics effort was started by this ALGOL '60
effort. But they didn't know what a model was, took time to work out.

If you think of the λ calculus as a programming languages, this doesn't quite work.

10 years later we had "Data Types as Lattices". Very good.

So there's 2 things in the air:
- λ as a programming language
- λ as a semantic framework

Late 60s early 70s, People started implementing the PL part
- ISWIM, implemented by "Birch", was really happy it ran in 50K
- PAL, from MIT, had types, polymorphic
- ML, from Stanford
- Scheme

> O: one thing that jumps out is how long it takes good ideas to arrive in practice

> M: I made a map of this in the early 00s. It was 20 years. But the gap
>    seems to be shrinking, I conjecture it's 10 years or less today.

Here's a funny program:
```
(λx. 3) [(λx. x x)(λx. x x)]
```

What should it do?
Everyone **knew** it had to be 3.
But every machine ran it forever.

What about this purity nonsense?
"Well those machines have bugs, they're impure, the failed to implement λ properly"

Eventually, Dave Turner built Miranda and it was actually "the right thing".
What triggered Haskell was that Miranda was trademarked as a fiscal property
by Dave Turner --- you couldn't do research on it unless you paid D. Turner :)

People came up with ideas, make a mess out of application order
- call-by-value
- head-normal form
- weal-head normal form
- applicative normal form

> J: what was the point?

> M: the point is trying to explain the connection between programming language
>    and calculi. There was this thing handed down by god in 1926 and we were
>    trying to understand our original sin, why none of the implementations
>    got it right.

Back then, programmers were lazy and didn't read journal submissions.
Well I was lazy too. Anyway if we weren't lazy we'd have seen that
Plotkin had a paper in '75 that explained all that. Totally explained it.

Here's what's in the paper, you'll fall off your chair:
- recursive evaluation
- abstract machines
- calculi, how they relate to the above 2 (so far this is just what I needed)
- observational equivalence (resurrected)
- compares calculi and observational equivalence, with translations
- translation to CPS
- the translations are not equivalent, not fully abstract, gee I wish they were

by the way, he uses step indexing in a proof

> O: kind of embarrassing really

> M: I believe that if you are interested in group A research, this paper
>    can be mined even further

Don't know why I dug up this paper, but I did. Went through at a rate of 1 page
every 3 days.

Then I found a book by Barendregt. At the start it said "we assume model theory,
category theory, ....." so I guess I have to read those too.

> O: but you're a grad student, you have all the time in the world

### Plotkin

To make things concrete, here's a syntax

```
e ::= x | λx.e | c e | n | add1 | sub1
```

Plotkin wraps up numbers, addition, subtraction as a `δ` set of primitives.
So this calculus extends to anything algabraic.

Here's β:
```
  (λx.e)e' =β e[x <- e']
```

reflexivity, transitivity, symmetry, compatibility with syntax.
Then you have a calculi `λβδ ⊢ e = e'`. (`λβδ` proves that `e` equals `e'`)

λβδ is just a name, it's the name of a proof theory, generated from the
β axiom, axioms about the constants (δ), 

> J: what's compatibility?

```
  λ ⊢ e = e'
  ----
  λ ⊢ λx.e = λx.e'
```

Some people call that referential transparency

Finally define eval (for closed terms `e`)
```
  eval(e) = n         if λβδ ⊢ e = n
  eval(e) = 'closure  if λβδ ⊢ e = λx.e'
  eval(e) = ⊥         otherwise
```

Now you can ask questions, like "is `eval` a function?"
Does this `eval` relate to ISWIM's SECD machine?

Oh by the way, this SECD machine is one of those horrible, buggy, original sin
machines that gets gods formulas wrong.

Theorem: this `eval` is a function
Proof: .... main lemma is Church-Rosser ....

Now we're beginning to see how the group A mathematics is CENTRAL to proving
things about a programming language operator.

`eval` is literally equal to an `eval` for ISWIM with a patch. The patch suspends
the computation.

That's good. How does he get there? By showing `eval` is equal to an `eval`
that uses a leftmost-outermost strategy.

Proof: .... uses Curry-Feys standardization ....

It doesn't prove everything, but you can use that equation to help.
This is great! The mathematical `eval` is extremely dumb. It searches for
a reduction strategy. Curry-Feys GIVES YOU a strategy. Compiler guys love this.
It's fast. Well, decently fast. Much better than search!

### Normal form
There's one thing that group A people didn't get.
Failed to see that normal form --- which is what mathematicians look for ...
.... if you get a closure as result, programmers apply it until they get a base
value.

Plotkin realizes, you have to fix a notion of value (which he considers as an answer)
and then he states βv
```
  (λx.e) v =βv e[x <- v]
```

this is for the λ calculus. NOT THE PROGRAMMING LANGUAGE.
You change this one ground axiom. Get a new calculus.

Then he proves the same axioms, and shows the same correspondence between ISWIM
(without the patch) and the βv calculus.

> M: there's a myth that call-by-value is a bug or mistake about gods perfect
>    equations

This framework has held up for a long time. About 50 years. Maybe now hitting
the limits of this thing with probabilistic programming.

Point is: this is math. It holds up. Works for implementing compilers.

Oh but it doesn't work for effects.

> J: so where does Haskell fit on your board?

> M: hm?

> J: where does Haskell fit on your board?

> M: hm?

> J: haskell

> M: hm?

> M: fine. Haskell was for a long time the invention of the purists.
>    Then they added monads and it's basically a different syntax for ML.

> O: with a horrible thing under the hood (call by need)


### Contination Thingies

So what we had before isn't really `call/cc`, we were inventing a new thing.
Grabs continuation and discards. `A e =^ C λd.e`, where `d` not free in `e`

```
 [left]   (C e) e' c- C λk. e (λx. k (x e'))

 [right]  v (C e) c- C λk. e (λx. k (v x))

 [top]    C e c-* e λx. A x
```

Only did this for call-by-value, left call-by-name as an exercise for the reader.

This was 1985? Sent the paper to a new conference, LICS.

Theorem: `eval_c` (eval with control) is a function.
Proof: ....
- that `c-` thingy is Church-Rosser
- c* is confluent
- consistent with β

was a really cool propaganda show

Theorem: `E = [] | v E | E e` play a special role,
         you can show people how continuations relate to λ calculi using contexts

A month later, I had standardization proved too and an idea for a machine too.

FDPC in 1986, I had 2 more theorems.
Theorem: `eval_c` is equal to `eval_c leftmost-outermost`
Proof: .... modification of Curry-Feys standardization ....

Theorem: the CEK machine defines `eval`

CEK machine was similar to SECD machine, but more like an interpreter. Why?
I couldn't figure out the math, for step indexing.
Because the only trick we had was to reify the continuation as an argument,
but then the step indexes don't work out.

Start with the LM/OM (leftmost outermost) operation, teased out the 
then teased out the environment.
Everyone recognized this as Reynolds recursive machine. So they were happy,
belived we'd satisfied Plotkin's program.

### Fiction
On the way to the conference, I worked out the same thing for state.
Key was to work the proof backwards.

(This is not the published truth, just a simple thing to understand)

Add 2 features to our expression language:
```
  (set! x e e)               --- the second `e` is a continuation
  (letrec [x = v] ... in e)  --- any number of [x = v] bindings
```

Now you can do things like:
```
  (letrec [x_i = v_i] ... in (set! x_i u e)
  =s
  (letrec [x = v] ... [x_i <- u] in e)
```

```
  E[(set! )] = set~ x e E[e']
```

In a top context, allocates a new pair `(x, v)`

need to make sure the `x` is unique, can't guarantee this unless you're in a toplevel context

worked backwards from reynolds, eliminating components


### Truth
What I really had wasn't `letrec`, I took the AST and locations in the tree
were an equivalence.

What if you set! a cell to itself? Then you have a loop on the tree. No problem.

And we had a total answer to Group A, state + continuations.

Plotkins framework really worked.


### Summarizing 

|                    | Name | Value | Control | State | State+Control |
| Values             |   x  |    x  |      x  |    x  |            x  |
| Axioms             |   x  |    x  |      x* |    x* |            x  |
| Machine            |   x  |    x  |      x  |    x  |            x  |
| Function-ness      |   x  |    x  |      x  |    x  |            x  |
| Efficiency (LM/OM) |   x  |    x  |      x  |    x  |            x  |

The State+Control column was my dissertation, 1987.
And Iwas happy, had a job, graduated, had a baby.

But as I'm driving I'm upset because of those `*`s in the table.

Did not allocate unique cells to set!-able variables.
But people still cite the POPL paper. 

Need to ack 2 other people:
- Ian Mason
- Carolyn Talcott

Both moved on to biological research, but they worked on this for 2 years,
in LICS '86 they presented a theory of control. Ian (the PhD student) was working
on a theory of first-order LISP.

There was a syntactic trick, that would help me with the `*`s

I sent an email, Bob Hieb said he had half of this and was hoping to make it
a dissertation.

control: `C λx. C e c- C λx. (e λx. A x)`
where `A` is "abort"

```
C e c- C λk. e (λx. A(k x))
```
Like an eta-law (not really don't get your hopes up)

With these two, you can prove away the "wart" with control.

For state, `(λx. e) v β! (letrec x=v in e)`

And if you've got ` letrecs next to each other, you can merge into one

You can separate effect contexts in a store if they don't intervene
(I missed the boat on separation logic, too bad)

SInce this improved on something in a conference, I only ever submitted to TCS.
Took 4 years in the print queue.
Bob Hieb actually died a month or 2 before it got published.

In the meantime, published papers on using this stuff
- logically reasoning about scheme programs (shallow, but solved puzzles from the literature)
- with Eric Crank, can tease out call-by-name and call-by-reference, 7 or 8 calculi for name passing

The effect? YAWN.  (For most of the PL community.)

The other effect was EXTREME DISBELIEF.
You can't possibly be serious! Only pure languages can be Church Rosser

> M: I said "go find the bug in my paper". And the funny thing is, there
>    were bugs in the paper. But I never heard back. (Fixed the bugs for TCS.)

### Types

In 1989, my second experience with types.

Working with the Russel language (I mean I'd seen ALGOL types but these are different).

Learned about type soundness "can you show that these types are preserved throught the computation",
thought about it for a while, oh yeah, all these operations work out.

Stared working with Mike Fagan & Corky Cartwright on Soft Typing.

Proposed to write a "throaway" paper with Fagan showing that types are preserved by all reduction relations
in the β-control-state world. I had kinda checked it, needed a student to do the history and all the details
and see if there was a use.

I mean, a use besides Bob Harper telling me types were good for things not going wrong.

Fagan said no. Waited. Eventually Andrew Wright came along.

> O: when was your sabbatical?

> M: years later, 4 or 5 years later

> M: everyone knew I disliked types. I said "you will become **analysis** people
>    instead of **building** cool programs" people. Totally right.
>    So yeah I went to the vatican of types to learn about them, because if you're
>    going to say you dislike something you should know what it is.

Andrew Wright came along. He worked on the problem.
The TR was buggy.
Bob saw the tech report, he loved it, send me a 3-line SML/NJ program.
I ran it, my machine went all black. Hadn't seen a core dump in 15 years.
then the machine starts up again. Oh cool! Must've been a bug.
I ran it again. Blew up again.

Turns out, they based the SML/NJ implementation after our proofs, but we were unsound.
They gave it that type, and the type let the program jump into the operating system
and leave things in an inconsistent state.

So that's how I learned about the bugs in the TR.

Anyway we fixed that. And we got Andrew's paper out.


### Progress and Preservation

Then I was popular. This was weird. I thought I was working on problems people
(group A) cared about. But they didn't care  Type soundness was great though,
people cared because everyone wanted to prove the consistency of their calculus.

THe story is similar to linear programming.
Dantzig invented these in the 30s, kept improving it and showing examples.
It wasn't until we had computers in the 60s that people finally realized how
useful linear programming is.

When you have an idea in science, just publishing the idea isn't enough.

You have to stick with the idea. Keep punching.

The paper had a one-line specification of GC:
```
  letrec x=v ... y=u in e
  =
  let x=v ... in e
```
for `y ¬∈ FV(v ..., e)

You can turn this specification into implementations, mark&sweep

TRUTH vs PROOF. Can just eliminate any variable you want, if it doesn't crash
then you're okay to collect it. Then you can see garbage collection is
reducible to the halting problem .....

Also see Goldberg from NYU: stop the program, infer types for all the bits.
If any have polymorphic type, those bits cannot possibly be used. Collect them.
It's very precise and very slow.

βv + control + state,
while traversing the AST leave handlers behind.
Then, at a "leaf", send a signal. Handlers propagate it to the top, and have a BIG 
handler at the top that does whatever it wants with the effects.

Sends '96, all you need to compare pure & impure is a projection.

This was also the beginning of algabraic effects. Had to be rediscovered 10 years
later by Plotkin & Smith.


### Referential Transparency

> O: always hated that term

There are 3 meanings you can extract from idealogues about this term:
- when you can extract `val x = 5;`, then `x` is always 5
- you can prove equations, `λ ⊢ e = e'`. But I just showed we can prove equations about imperative programs
- you can substitute equals for equals, replace `2 + 2` with `4` everywhere!
  (really? so if I want to print "2+2" its the same as "4")

Quine defines the idea of referntially transparent **positions** in a language.
Eventually they admit that they want observational equivalence. ....

Bottom line, referential transparency is just a slogan

