# Title : From partial evaluators to JITs
## Speaker : Oli Flückiger

### Background on Partial Evaluation

1936, Kleene's `Smn` theorem.
Let `f` be a function of `m + n` arguments.
Given `n` values `v1 ... vn` there exists a function `g` such that
`g ~= f v1 ... vn`.

> A: the theorem is obvious in the λ calculus, it's usually stated in the
>    context of Turing machines, Post systems, etc.


### Example of partial evaluation

```
let pow(x,y) =
  if y = 1
  then x
  else x * pow(x, y-1)

let sqr(x) =
  pow(x,2)
```

Partially evaluating the call `pow(x,2)` with respect to the argument `2`
gives `pow(x,2) = x * pow(x,2-1) = x * x`.

Easy!


### Relation to Programming Languages

1971 Futamura, the three Futamura projections.
Let `I` be an interpreter;
let `C` be a compiler;
let `P` be a program;
let `D` be some data.
Let `α` be a (mysterious) partial evaluator.

**(0)** `I(P,D) = C(P) D`
(Running an interpreter on some program and its data gives the same result
 as compiling the program, then passing the data to the binary.)

**(1)** `α(I,P) ~= C(P)`
(Partially evaluating an interpreter with respect to a fixed program
 produces a binary for the program)

> B: what is the "squiggle equal"? Equal but for performance?

> C: the right side is usually more efficient

> Oli: I wrote `~=` because we're not comparing values

> D: wait why not just use `=`?

> A: this is a common misunderstanding, that values and functions are somehow
>    different. The equality you want is extensional equality, comes from
>    set theory. This is one of those cases where computer scientists can be
>    incredibly dumb. If you ever want to prove your JIT compiler is correct,
>    you will need a precise notion of equality. Very important to distinguish
>    syntactic and extensional equality.

Next level of Futamura: specialize the partial evaluator to the interpreter.

**(2)** `α(α, I) ~= C`
Tailoring the partial evaluator to the interpreter gives a compiler: a program
that accepts programs and produces binaries.

**(3)** `α(α, α) ~= Q`
where `Q` is a compiler generator.

> E: implications?

> Oli: people were very enthusiastic. Mix claims to be the first implementation
>      of Futuramura's projections. But no implementation has been as fast as
>      a hand-written compiler. And the partial evaluator for the second and
>      third levels look like compilers anyway. Very complex, lots of low-level
>      parts to generate efficient code.
>
>      I'd say this research area is a dead end. Building a compiler from static
>      descriptions of programs and interpreters is too hard.

> E: then it (Futamura projections) didn't work?

> A: yes it did! it's elegant theory! but it didn't work in practice.

> F: this project / dream sucked up some of the brightest minds

> A: shows one more time that theory is seductive and useless

> G: is it all useless? or only some of the equations?

> Oli: most of the failed efforts were on the 2nd and 3rd projections.
>      There's been recent interest in the first projection, specializing
>      interpreters.

> F: compiler compilers

> A: this happened (Futamura's work) just as denotations semantics as a
>    description for compilers was dying out. Futamura revived interest.
>
>    just as grammars are a uniform description of syntax, denotational
>    semantics was hoped to be a uniform description of meaning
>
>    problem: need to denote what's known ahead of time

> F: Neil Jones was a leader there

> A: ... analogies ... look at program and data know already ... see EOPL
>    for an example

> Oli: so, it the problem similar to being given a program using templates,
>      then automatically reconstructing what's template and what's code?

(yes)


### 2001 Dynamic Partial Evaluation (Sullivan)

Jumping over the misguided efforts.

> F: no not misguided! lots of smart people made this mistake

> A: I was lucky, found early on in my dissertation .... just because a
>    language has beautiful meta-theorems doesn't mean the language is useful.

> F: though, I do use that "elegance" metric a lot

This paper was builing off meta-object protocols. We're not going into that,

> F: static reasoning takes a holiday

> B: how is this different from R?

> A: NO NO NO

> E: meta-object protocols give programmer control of the language, 
>    but there is a language involved. R is just a hack.

Here is one rule from their partial evaluation semantics

```
  p(x) = v
  -----------  v : τ
  x p τ => v
```

(Intermission: we argued a lot about the greek letters in the rule above.)

Idea of dynamic partial evaluation ... extend the values, record static info
we have about the values, as types.

Extended values `⟨ v e τ ⟩`
- `v` is a value
- `e` is a residual expression
- `τ` is the type of `e`

> B: this is wrong

> Oli: let me give an example

> A: residual has a technical meaning "followed"

Example rules (note: every value is a singleton type)

```
   p(x) = ⟨ v e τ' ⟩        τ' <: v
   ---------------------------------
   x p τ => ⟨ v [v] τ' ⟩

   p(x) = ⟨ v e τ' ⟩        τ' </: v
   ---------------------------------
   x p τ => ⟨ v e' τ' ⟩

   e0 p τ_Bool => ⟨ true e0' True ⟩
   e1 p τ => ⟨ v e1' τ' ⟩
   -------------------------------
   (if e0 e1 e2) => ⟨ v e1' τ' ⟩

   e0 p τ_Bool => ⟨ true e0' τ0 ⟩
   e1 p τ => ⟨ v e1' τ' ⟩
   -------------------------------
   (if e0 e1 e2) => ⟨ v [if e0' e1' e2] ⊤ ⟩
```

Note in the last rule, the final type is "Top" (⊤). Sound overapproximation.

> A/G: ah okay, so the "residual" is the derivative of the term on the left
>      with respect to the environment and type.
>      And the type in these triples is the most precise sound approximation.

> Oli: yes that's the idea

> E: how do they do the specialization?

> Oli: the meta-object protocol gives a hook. Think of generic methods,
>      you can dispatch to a faster version.

> E: sound?

> Oli: they argue that the `(_ p _ => _)` judgment always produces equal terms
>      (bg: deterministic with respect to `p`?)

> G: is the new term faster?

> Oli: not sure and it seems expensive to compute the new term. Point is, this
>      paper was a model, a specification to guide later implementations.

### PyPy paper

> F: it's surprising how often the popular paper that everyone knows and
>    cites isn't where the idea originally came from.

> E: helps to have a good title

> F: helps to have a more readable presentation

> F: Fisher, John Ellis ...

> A: the key idea is to apply tracing to the one program we ALL care about,
>    the interpreter.

Local optimizations can get rid of PC manipulation, bookkeeping.

PyPy needs user annotations on "important guards" that can't be compiled away
and back-edges where traces might start.

> E: what about exceptions? They complicate control flow. Do they complicate
>    the tracer's fast paths?

> Oli: we're optimizing the interpreter (host language) and not the program
>      (guest language). The guest can throw exceptions.

> E: which Futamura projection does PyPy correspond to

> Oli: doesn't exactly fit. Somewhere between `α(I,P)` and `α(I P D)`
>      How about `α(α(I,P), D)`


### 2013 OOPSLA Truffle One VM to Rule them All

This paper had a number of authors.

> E: a modest goal

I believe they took inspriation from the FastR project

> E: Other way around --- FastR was inspired by Truffle. FastR just got
>    published first.

Truffle argues: writing an interpreter for the bytecode of a language is a bit
too far removed from the language itself. Want to optimize AST interpreters.

> Oli: the host language probably needs an object system

Example: the function `+`. It can take many different inputs, Truffle
keeps a lattice of these input types. Say:

```
    Uninitialized____
     /        \      \
  String     Int  --- Float
        \  /     ____/
         Generic
```

As values flow into the `+` function, Truffle specializes it from:
- nothing (uninitialized)
- Int addition (fast)
- generic addition (checks whether arguments are Ints or Strings)

The interpreter throws exceptions to invoke the rewriter

> A: why use exceptions? In a sense you're revisiting the phase where
>    you resolve binding information. They picked a poor language to explore
>    this idea.

> F: Ole Aggesson .... CPA ... idea comes up repeatedly


> Oli: ok I should say this "types lattice" is upside-down from what you
>      normally see

> F: luckily flipping a lattice gives you a lattice


> E: is this a "method JIT" or a "tracing JIT"

> Oli: a method JIT. Tracing is replaced by a self-adapting tree.
>      compiler has some basic tricks to update and fall back

Truffle requires some annotations .... about where to store local variables?

> E: the start of the talk said partial evaluation was a failure. But Truffle
>    is a success?

> Oli: yes a success, but the partial evaluation is disguised. You have a
>      program tailored to the types flowing through it.

> E: close to the Sullivan idea


> H: who writes the different versions of `+`?

> Oli: the programmer, but Truffle gives you some help


> A: as you see, Cousot invented everything, everything static is an abstract
>    interpretation. Futamura invented everything dynamic, it's all partial
>    evaluation.

> G: didn't Reynolds invent both of those?



> A: part of your training as a phd student is leearning to communicate your
>    technical ideas

