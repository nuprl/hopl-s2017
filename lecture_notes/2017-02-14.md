# Title: Linear Types for Low-level Languages
## Speaker: Daniel Patterson

Goal today: show how linear types can help make programs run more efficiently.

This is active research! A bunch of "recent" commercial and academic languages
use linear types:
- [Rust](https://www.rust-lang.org/), see the recent
  [keynote](http://popl17.sigplan.org/event/popl-2017-papers-rust-from-popl-to-practice#modal-phid_a19ef757dfad82a6b45b08606914533d)
  ([pdf](./resources/t-popl-2017.pdf)) at POPL 2017.
- [Cogent](https://arxiv.org/abs/1601.05520), see the recent
  [paper](http://icfp16.sigplan.org/event/icfp-2016-papers-refinement-through-restraint-bringing-down-the-cost-of-verification)
  ([pdf](http://people.mpi-inf.mpg.de/~crizkall/Publications/cogent.pdf)) at ICFP 2016.
- [ATS](http://www.ats-lang.org/), developed by Hongwei Xi's group at Boston University.
- [Cyclone](http://cyclone.thelanguage.org/)
  ([wiki](https://en.wikipedia.org/wiki/Cyclone_(programming_language))),
  no longer supported, but was previously maintained by Greg Morrisett's group at Cornell.
- [Vault](https://swehb.nasa.gov/display/7150/Vault), previously maintained by Manuel Fähndrich.
  See the [PLDI 2001](./resources/df-pldi-2001.pdf)
  and [PLDI 2002](./resources/fd-pldi-2002.pdf) papers.

> M: (about Rust's keynote) you know what Landin said: as soon as the
>    theoreticians get ahold of an idea, it's all downhill.

> J: why did Cyclone fail?
> D: good question
> J: maybe **the** question (perhaps, something that cannot be overcome)
> D: well Rust credits Cyclone as a major influence
> M: academic projects don't last. That's it.

### Fundamentals

#### Sequent Calculus

Girard introduced linear logic in a 1987 paper.
According to Girard,

> computer science prompted the subject through semantics

i.e., resource usage mattered in computer science, so Girard developed a logic
to reason about resource usage. In the same paper, he also remarked:

> Linear logic will therefore help us improve the efficiency of programs

Since the beginning, linear logic has been about solving practical issues
in computer science!

> W: Did Girard even know what that sentence meant? M., do you want to chime in?
> M: no comment
> D: anyway, this is the paper people go back to
> M: well that's because Girard was engaged with the CS community. Much more
>    than any other logician.
> N: hmm, wasn't the linear logic paper published in a CS journal? (yes, TCS)

Linear logic's roots are in Gentzen's 1934 work on the sequent calculus.
People definitely realized the sequence calculus had impliciations for programming;
also people were connecting logic to programming earlier in the 1930s.

> M: when we give credit to Kolmogorov etc. for connecting logic to programming,
>    I think we are a little too generous because they did not explicitly
>    connect logic to work on partial recursive functions. That was a non-trivial
>    step.

> X: wait! you have to talk about the subformula property

Right, here's the cut rule (with both propositions and language terms):

```
  Γ ⊢ t : A   Γ,x:A ⊢ u : B
  -------------------------
  Γ ⊢ u{t/x} : B
```

Gentzen's "key lemma" (German: Hauptsatz) is that any proof using the cut rule
can be translated to a proof that doesn't use the cut rule. This is a key lemma
because:
- with it, Gentzen could prove that the sequent calculus is consistent
  (by showing there is no way to prove a contradiction without using cuts)
- cut elimination implies a _subformula property_ --- that one can build a
  larger proof from smaller proofs, just as one can build a large program by
  composing functions

#### Linear Logic

Linear logic is similar to the sequent calculus.

Biggest difference is the absence of structural rules:
- no _contraction_, cannot duplicate assumptions in the context
- no _weakening_, cannot discard assumptions in the context

When proving `Γ ⊢ A`, every fact in `Γ` must be used _exactly once_.
Consequently, the sequent calculus rules for products no longer work out (would
need to discard/duplicate propositions). Instead, there are 2 kinds of products
in linear logic:
- "tensor" (`A ⊗ B`) : to eliminate a tensor, must use both facts at once
- "with" (`A & B`) : can project either conjunct, but to introduce a "with" need
  to prove both sides simultaneously

Girard also has an "of course" operator (`!A`). Contraction and weaking holds
for "of course" propositions. If you stick to just "of course"s, you're back
to intuitionistic logic.

> M: question: is ! analogous to monads, in the sense that it introduces a
>    controlled way of escaping to intuitionistic logic?
> N: yep, it's a co-modality and monads are just modalities. It's a co-monad.

### 1988 LaFont

_The Linear Abstract Machine_

> M: I remember reading that paper!

Idea: build a computational model of linear logic

> W: wait, can we go back to linear logic? Since we just added intuitionistic
>    with "of course", how do we encode "bottom"?
> N: the axiom `Γ, 0 ⊢ A`. (0 is the encoding of ⊥)

> J: back to code?
> M: hah! I remember reading this _Linear Abstract Machine_ paper and it was
>    such a letdown when I finished

The linear abstract machine is defined with a transition function from states
to states.
```
 ⟨ Code , Env , Stack ⟩ → ⟨ Code' , Env' , Stack' ⟩
```

And the system has some interesting properties:
- doesn't need a garbage collector, it's safe to deallocate a value once its used
- can add mutation / side effects without screwing anything up

> M: I did that --- added side effects --- when I first read the paper.
>    Those "side effects" wind up being completely trivial.
> D: yep, since you're the only one who can mutate anything, it's basically
>    the same as making a copy
> D: we'll see that the recent focus (post 1990) has been in bringing these
>    desirable features to a usable programming language.

> R: does the machine have "of course"
> D: not sure ... oh yes, it does. Implemented by copying. There is a lot of
>    copying


### 1988 Holmstrom

_Linear Functional Programming_

This paper is cited, but I couldn't find a copy. Even emailed Simon Peyton Jones
(an editor for the workshop) and he said his shelf for that conference only
went back to 1993.

> M: that phenomena you just experienced is "archival" vs. "ephemeral"
>    publications. Journal papers are archival, they stay in the libraries
>    for ever. Workshops and conferences are ephemeral, and every `N` years
>    (usually 10) they're thrown in the garbage.

### "par" operator

Earlier we saw the "with" conjunction. The dual of `A & B` is `A ⊕ B`. Here's
one pair of their intro ("right", R) and elim ("left", L) forms.

```
   Γ ⊢ A   Γ ⊢ B         Γ, A ⊢ C    Γ, B ⊢ C
   ------------- &-R     -------------------- ⊕-L
   Γ ⊢ A & B             Γ, A ⊕ B ⊢ C
```

(Note: the duplicated `Γ` above the line isn't sharing/duplication,
 think of it as "simultaneously prove")

We have 2 kinds of conjunction. What's the oppositite of "tensor" ⊗ ?
It's the "par" operator (`⅋`). Here are the par rules, note we have 2
simultaneous conclusions in the intro form:

```
  Γ ⊢ A,B      Γ, A ⊢ C    Γ, B ⊢ C
  ---------    --------------------
  Γ ⊢ A ⅋ B    Γ, A ⅋ B ⊢ C
```

> J: truth in advertising, we're not ever getting to 'low level programming',
>    are we?
> M: this is low level! λ calculus!


### Negation

With 2 simultaneous conclusions, we can talk about negation
(BTW all this is from the Girard paper)

```
  ----------
  ⊢ A^⊥, A
```

And with negation, we can define the "lollipop" operator

```
  A -o B =(def)= A^⊥ ⅋ B
```

> M: okay what's the _idea_ behind all this?
> D: ... (begins to reply) ...
> A: Stop! We're running out of time, got to switch to programming!

Two more facts:

```
  A ⊢ A
  ---------
  ⊢ A, A^⊥
  ---------
  ⊢ A ⅋ A^⊥

  X,Y ⊢ Z ⅋ Q   "and"    ⊢ X ⊗ Y -o Z ⅋ Q
```

> C: what does a comma (,) on the right mean?
> D: that you proved both things (`A,B`) simultaneously, using the same resources
> T: ??? that makes no sense
> M: , is a coroutine
> D: interesting fact, you can prove `A → ⊥` and `A` at the same time by
>    waiting until someone gives you an `A`. So like, if someone asks you for
>    an `A` you just block until someone else calls your continuation.
>    Critically, you know each thing (ask for `A`, call `A → ⊥`) is going
>    exactly once!


### 1990 Abramsky

but published in 1991 / 1992, and the main publication is 1993, _Computational
Interpretation of Linear Logic_.

> M: the print queue for journals was so long back then

goal was to use linear logic fo parallel computation,
have many co-equations

```
  x1 ⊥ x2, ... , + ⊗ M
```

reduce `x1` to identifier and `x2` to value, substitute

Identifiers are like channels:

```
  t ⊥ x , x ⊥ u
  →
  t ⊥ u
```

```
  t ⊗ u ⊥ t' ⅋ u'
  →
  t ⊥ t' , u ⊥ u'
```

> W: this is not symmetric, in tensor and par
> D: nope, there are symmetric rules but this → is reduction


### Lit. Survey

Done with technical matter. Key points is that linear logic lets you:
- do things in parallel
- and you don't need a GC

#### (1989 Guzman Hudak)

Tried using linear logic, foudn it too slow, made something much more complicated


#### 1990 Wadler _Linear Types can Cahnge the World_

Very popular paper. Noted that copying is a big problem. Too much copying in
linear logic.


#### 1991 Mackie _LiLac, a functional language based on linear logic_

Masters thesis, added things like abstract datatypes so you could actually use
this as a programming language.

In the end, needed a GC.

Noted, it's very annoying to need to reason about the correctness of your
algorithm **and** the linear logic rules. Have to do it all simultaneously.

> M: at the time, functional programming was seen as this panacea. You just
>    write what you want, and don't worry about allocation, details of the
>    machine.
>    Transformational Programming (TP): take a 3 line functional program and
>    map it to the 100,000 lines or whatever of assembly. Resource management
>    was very important to people working on TP. That was the context for
>    linear logic.
>
>    translate elegant functional programs to efficient imperative programs

"Low level" referes to the idea that programmers would consider resource
management --- and not just the solution to the problem at hand.


#### 1992 Baker _Lively Linear LISP_

No GC, reasonable performance (demonstrated via benchmarks). Short paper.

#### 1992 Lincoln, Mitchell _Operational Aspects of LL
#### 1992 Chrimas Guntner Rieche _Ref Counting as Computational Interpretation of Linear Logic_

- Both papers have "of course" (!)
- Both have "linear" references that can still have multiple pointers to them.
- Both sometimes have to move "linear" values to the GC'd heap for "of course"
  variables

#### 2000 Hofman _A Type System for Bounded Space & In-Place Update_

Uses a token to represent a resource. For instance, `cons` needs 3 arguments:
one resource token and 2 values to put in the cons pair.

> J: must be pretty restrictive, can't make the size of a data structure be
>    a parameter
> D: yep, and it's a first-order language

#### 2002 Cheney Morrisett _Linear Typed Assembly Language_

This one did not work out (unpublished), but it happened and it led to Cyclone.

#### 2004 Ahmed Fluet Morrisett _L3: A Linear Language with Locations_

- had unrestricted pointers
- but restrictions on their use

enabled some useful patterns

Rust has a borrowing system that allows local aliases (kinda similar?)

### Wrapping Up

Literature theme:
- linear logic has nice properties
- seems like a principled system
- so how do we make an implementation work?

> M: back when I taught a course in Rust, J and I had an interesting
>    conversation about why this type of programming will never work.
> J: Well I don't remember the details of that conversation, but I do remember
>    talking to Grossman about Cyclone. The issue is that the programmer needs
>    to think about too many things ... the libraries need to get it exactly
>    right ... and if things aren't fixed in the right way, the consequences
>    percolate down to every client program.
> D: right, Mackie observed the same problem of synchronizing everything.
>    Tov's "Alms" paper was about giving clients more control over the API.

> M: the problem is, no more λ! It's no longer possible to abstract over
>    anything, you need to copy code for these borrowing things ... chafing
>    against programmers ability to abstract ... and in this day and age even
>    mediocre programmers want to abstract
> D: polymorphism over regions helps a lot in Rust
> M: and the eventual solution is "unsafe" blocks ... but I personally found
>    Rust okay. Far more doable than I expected going in.
> D: and it has this nice property* of "no data races"
>    (* can't say "proof" because it's not proved. The Mezzo language has a
>       proof though.)
> M: boils down to the Laffer curve of type systems

(B: on 2017-02-14, I found 5,600 occurrence of "unsafe" in the Rust code
 in the Servo project. This is after filtering out comments, but still an
 approximation. And it was maybe 90 `.rs` files?)

