# Topic: Higher-Order Flow-Analysis
## Speaker: Olin Shivers

### Intro / Motivation

"Higher-Order Flow Analysis" is a better name (conveys an insight) than
"Higher-Order Control-Flow Analysis"


Start with data-flow analysis. It's a great idea, big hammer.
Looking for path-invariant facts. Fact ~= theorem.

> There's a flowchart on the board. Olin says he used to write BASIC
> programs by first writing a flowchart.

Example problem: available expressions. At line N, what expressions in the program
have already been computed? (Have their value in a register, variables in the
expression haven't been mutated.)

> Olin: Every time through the loop, I've saved myself an "add". That's an optimization.


### Program Analysis by Analogy

Rice's theorem is great. "Any interesting question about Turing machines is unsolvable."
Forget the halting problem, Rice's theorem is way better.

> M: wait, what you're saying is you can't build an algorithm. But since logic
>    is stronger than programs, you can reason in a logic about the problem.

Very hard to find truths about a program.
Suppose the space of all truths is a complex polygon. You **could** use logic
to describe this space exactly, with a very big formula. Another way to describe
the space is by enclosing it with a circle.

Polygon is exact and very complicated.
Circle is an approximation, but sound and very simple (origin, radius).

> Olin:
>   It's not like court .... not "nothing but the truth"

(More geometry, suppose you have a complicated polygon and you want to find its
 area. There are at least 3 choices:
1. measure the exact area of the polygon ("the whole truth, nothing but the truth")
2. draw a circle around the polygon, measure its area ("the whole truth")
3. draw a circle inside the polygon ("nothing but the truth")

Choices 2 and 3 are easier to do, but imprecise. How imprecise? Depends on
the shape of the polygon and whether the circle is a tight approximation
(circumscribed, inscribed).)


> Olin:
>  Another way to look at it is: how are you resolving the things you can't
>  figure out.

> M:
>  question here is whether you believe models exist; if so you can ask
>  whether "it" is true or not. If you only believe in construction, you can't
>  say whether its true or false. Industrial things are always circles on the
>  outside.

Computable approximation is the name of the game.
That's what math is too, finite symbolic things that talk about infinite sets.


### Lattice of Information

When you're playing the game of program analysis, you use a lattice of information.
"More information" goes up. Each point is something "you know" about the program.

Ordering is "consistent with, but weaker".

Flow analyzer is a "weak method" (term from artifical intelligence).
It's a theorem-discoverer rather than a theorem-prover.

> M:
>  In my world, you don't want ⊤ because it represents inconsistency.
>  In program analysis, ⊤ is acceptable because that's when you approximate.

Start with what "you know" about the program. Infer more, conservatively.
Eventually, you don't learn anymore.

> M:
>  Olin is using the word "implies" in 2 senses. One is the logical "x implies y",
>  the other is moving up this lattice in terms of the ordering. These
>  are almost the opposite.

Also beware, people tend to flip ⊤ and ⊥ on a lattice. So be careful when
you're reading. The only solution is to go slowly and reason carefully.

Anyway, all of program analysis is start from "no information" and iterate to
a fixpoint (go up, up, the lattice).
Another way is to start with "everything". Then locally carve away beliefs
and come down the lattice.

("no information" corresponds to `⊥`, "everything" corresponds to `⊤`)


### Non-termination

There's a problem with this (Kleene iteration) method, the lattice might be too
large. Maybe you keep iterating, learning more information, and never stop.
Hashtag Zeno's paradox.

All this is summarized in:

```
   Monotone Data-Flow Analysis Framework
```

for Fortran. Give me a flow graph and a space of truths and a way to modify
these truths with respect to basic blocks.

> Max:
>  This framework works for programs without loops too, correct?

> Olin:
>  Yes

> Max:
>  (bg: some followup question, I missed it)

> Olin:
>  It's about the granularity of your fact set.
>  Say I want all blocks in the program where `s+4` is available at the top.
>  Start with "no blocks", 

"Meet is down, join is up"

Lattices can be infinite (flat lattice of integers). That's no problem.
But you really want **finite join/meet paths** (which one depends on the
analysis you're doing --- you want at least one or the other).

```
  Flat lattice of integers

          ⊤
       /  |  \
  ... -1  0  1 ...
       \  |  /
          ⊥
```

> Jan:
>   How do you know you never go "back down" in information?

> Olin:
>   Ah! That's the first word in "monotone data flow analysis"

> M:
>  Just to relate this point to what I said in the beginning, 
>  the functions that are 'meaning' functions need certain properties.
>  The first property I gave in my lecture was 'monotonicity'
>
>  When your domain is flat, you have a first-order language. When it
>  has height, you have a higher-order language

(bg: higher-order **language** can't be right)


### Array Indexing

Here's another analysis, statically checking the bounds of array indices.
Want to eliminate run-time bounds checks by tracking the range of integer
variables.

Suppose we have 2 pieces of information from 2 paths.

```
  [lo, hi)   and  [lo', hi')
```

Join these, we get

```
  [min(lo, lo'), max(hi, hi'))
```

A more precise abstraction is to keep a set of possible values for each integer
variables.
Careful though, that representation might "explode" on you when you run the
program analysis.


> Olin:
>   Behind every representation there is a theorem.
>
>   Every time you come up with an information lattice, you come up with a little
>   theorem prover.


### Higher-Order Case

That's enough classical flow analysis.
It's just not interesting until you do something higher-order. First-order
languages suck.

(bg: we have slides now)

Higher-order languages: in order to do flow analysis, you need a control-flow
graph. In order to get a control-flow graph, you need to do flow analysis.

> Olin:
>  I slammed into this problem in a brutal way. As a first-year grad student
>  I was officially into artificial intelligence but I was also a scheme geek.
>  (Advisor: Alan Newell.) At Yale, I got involved with the T team,
>  Jonathan Rees, Richard Kelsey, Richard Krantz, Jim Philbin.
>  This guy in Silicon Valley invited the team out to Silicon Valley.
>  All my buddies were gonna go out there and build the fastest functional
>  compiler.

> M:
>  They beat Pascal! It's not just the fastest functional, it was the fastest!

> Olin:
>  That was serious Pascal too, a whole OS written in it. Krantz ORBIT numbers
>  were better than the Pascal numbers.
>  Anyway I went with this team to California. We're dividing up responsibilities,
>  I'd been reading about data-flow analysis and I KNEW nobody had done it for
>  a functional language. Nobody else had the glory, I figured I'd grab the
>  glory. So I said "I want flow analysis". They said okay.
>
>  All summer I failed horribly. I went to work every day for 8 hours and failed.
>
>  I knew all this flow analysis, but we didn't have a CFG and didn't know how
>  to get it. In the end I didn't contribute a single line to the compiler.
>
>  Back at CMU I worked on the problem in secret, under the cover of being
>  an AI grad student. But I didn't do any work.
>  Really only at CMU with these huge DARPA grants would they have been able to
>  keep me around. On Black Friday, Alan would go in and say "Olin hasn't done
>  anything, but I think he has potential."
>
>  After a long time, I thought I had something on this CFG problem.
>  So I made some appointments with the PL faculty. Peter Lee, Dana Scott,
>  John Reynolds. Pretty awesome, right?
>  I didn't know who these guys were. Anyway I told them I had an idea how to do
>  flow analysis. Didn't pretend that it was correct or would work. I wanted to
>  see if they'd nod, shake their head.
>  All five nodded, said that should kinda work. I had a thesis topic.
>
>  Overnight I was a hero --- "Olin's independent, he did all this by himself."
>
>  This is not how I recommend doing grad school. Totally backwards.

> M:
>  I read Andrew Appels thesis on my flight. Didn't miss a word. There was
>  nothing in it. So I want to say, there are 2 approaches to (writing a dissertation)
>  One is to write crap, make sure your advisor understands. Get it out and
>  get famous afterwards. Other approach is to spend 10 years writing a beautiful
>  dissertation.

> Olin:
>  I just didn't quit because people in Atlanta thought I was a genius.
>  Figured I'd stick around until they kicked me out.
>  I don't recommend that as a strategy for gradschool either.


### Control-Flow graphs for CPS programs

CPS is continuation-passing-style. Point is, you encode everything with λ.

> "The fox knows many things, but the hedgehog knows one great thing."
> Olin:
>  Archilocus was clearly a guy who experimented with pharmeceuticals.
>
>  Two styles of compilers for functional languages, focus on λ or focus on let.
>  The ANF guys use `let`. The Haskell guys use `let`.

> M:
>  The `let`, just like `λ`, comes from a calculus.
>  The `let` really comes from monadic calculus.
>  ... M-calculus ... Moggi ...

> Olin:
>  λ is a dessert topping and a floor wax, it's good for everything.
>  This was Guy Steele's thesis.

CPS imposes uniform representation. All transfers of control are procedre calls.
Control flow is given by a function L:

```
  For each call site c in program P,
  find a set L(c) containing all λ and primop that could be called at c
```

No syntax for conditionals, just function with 2 continuations.

In industry, need to separate your user-functions from your internal functions.
No time to say more.


### Tool 2: abstract interpretation

Cousot & Cousot.

1. mathematically describe your language
2. tailor the description to your property of interest
3. that's the gold standard, but incomputable. Build a computable approximation.
4. relate the models, prove correctness


### Contours

Lots of great research comes from tastefully assembling things other people
thought of. Here's something I had to figure out on my own:

Need to split your environment into 2 pieces.
- Contour environments : λ-binder -> contour
- Variable environments : variable × contour -> value

Contours are "magic cookies". No serious meaning, but they're distinguishable
and there's an infinite set of them.
Every time control enters a λ, allocate a new contour.
(Contours are **red numbers** on the slide. Emphasizes "no serious meaning".)

Contours are how to distinguish multiple bindings of the same variable.
(e.g. closures?)

The variable environment gets tagged with contours. Now you know what variable
binding to reference at a given program point (represented by a string of λ-labels)

### Technicals

Assume unique labels for call sites and λ expressions.

Primops have internal control flow, need to introduce internal call sites
for where primops call their continuation (return).
Primops like `if` have multiple internal call sites.

Syntax ... programs is a (λ (halt) ...)
... have variables, primops, constants, function calls.

Semantic domains:
- basic values are integers + false
- procedures are closures, primops, and "stop"
  - closures are λ and a contour environment (label -> contour)
- variable environment is a global table.

Semantics are denotational


### Aside

In CPS, denotational semantics and smallstep semantics are the same small machine.

You don't understand the shift that happened when Matthias chucked his PhD at
the world. It was a difficult clever useless trick. Here's something you can
do with smallstep semantics. People thought Matthias was clever, but didn't
really care / understand. Because Dana Scott. Dana Scott used denotational
semantics and that was that.

Then there was a problem. Type safety proofs.

> M:
>  Maybe you should get back to your presentation.

It was a **complete** 180 degree shift ..... (and so on) ... everyone realized
type safety proofs could be much easier if you did them Matthias's way.

> M:
>  As an embarrassed audience member, I can say this isn't the worst.
>  The worst was when the presenter said 'You KILLED denotational semantics'.


### More Technicals

Denotational semantics for evaluating arguments, calls, λs.

> M:
>  When you look at this F applied to the first value,
>  when you go back to where it comes from, the second line of the C function,
>  now you go back to the A thing, all A gives you is l β,
>  Where does all the stuff in the quine quotes come from?
>
>  Whats really happenening is these labels, you have unique labels so you
>  can look up anywhere.

(bg: these are bad notes, point is that the labels uniquely identify λ in the
 program, they make the math simpler but they are tricky to implement)

> J:
>  ... where is the control stack? ...

> M:
>  When you do CPS, there is NO call stack.

> Olin:
>  You are encoding the context in λ

> M:
>  He said, this is the stack frame, which means the stack becomes part of the
>  internal program representation.

> Olin:
>  You can add a stack to the model, but that's D. Vardoulakis dissertation,
>  you're 20 years ahead of me.

Don't compute actual result, compute a call cache.
(This is part 2 of "the Cousot approach")

This is incomputable. It's perfect.

Finite program, infinite environment structure.

Where did the infinity come from? Go back to the semantic domains.
It's the function calls, were generating a new contour at each call site.
The environment structure is infinite, and that's what's making our analysis
incomputable.

Going to abstract contours to a finite set.
Accept some imprecision.

At each call site, now need to choose a contour.
How to choose?

- 0-CFA : there is 1 contour for each λ, always pick the canonical one
  all bindings of that λ variable get combined into a set
- 1-CFA : index contours by call-site. For each call site to a λ, you have one
  contour
- K-CFA : use `k` call sites as the index
- Wright & Jagannathan : let polymorphism determines contour precision
- CPA : contour is parameter **type***
  - forwards = Agesen
  - backwards = Spoon
  - gives pretty good flow analysis because the types of outputs tend to depend
    on the types of inputs

> Olin:
>  when I saw the CPA idea, I could have shot myself

Can use these contour models in direct-style, or in ANF.


### 3 minutes left

This stuff has limits. Since you have to merge, you can't ... ?

Matt Might: we had a compilicated abstraction. Did a simple version first.
While doing the simple version Matt had 2 cool ideas, counting and GC.

The simple ideas were so good, everyone ignored the complicated abstraction.

Suppose I have 2 sets, and I know 2 values are in the sets.
  x ∈ A, y ∈ B
Furthermore discover that A = B. Learn that they're in the same set, but can't
learn that `x = y`. But if you learn A and B are singleton sets, you learn
that `x = y`.

This lets you do strong updates. Keep a counter, ℕ + {∞},
(bg: somehow this gives you singleton sets (if the counter's not infinite))

Remove the "dead crap" to keep the sets small. Garbage collect the variable
environment & other structures.

Okay that's like 2/3 of Matt's PhD

Big picture here, this is hard because of "the centrality of λ".
There are 3 fundamental structures in a program: data, environment, control.
And λ is all 3. They all meet and intertwine. If you want to analyze control
structure and you have λ on your hands, you need to solve all 3 problems.
That's why I hit a brick wall at ORBIT, you HAVE to do them all together.

> M:
>  This also exists in first-order languages, with the dataflow graphs.
>  Control is CFG
>  Env. is interprocedural analysis
>  Dataflow is in ... dataflow graph?

Next link: Abstracting Abstract Machines & Vardoulakis.

